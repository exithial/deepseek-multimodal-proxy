# L√≠mites de Contexto de Modelos

## Modelos Ollama Locales

### Qwen2.5:7B-Instruct
- **Contexto m√°ximo**: 131,072 tokens (128K)
- **Generaci√≥n m√°xima**: 8,192 tokens
- **Fuente**: [Documentaci√≥n oficial Qwen2.5](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
- **Caracter√≠sticas**: 
  - Soporta hasta 128K tokens con YaRN
  - Mejorado para seguir instrucciones
  - Excelente en generaci√≥n de texto largo
  - Soporte multiling√ºe (29+ idiomas)

### ‚ö†Ô∏è Nota: DeepSeek-Coder Eliminado
- **Estado**: Eliminado de la compatibilidad con Ollama en el proxy
- **Raz√≥n**: Simplificaci√≥n del stack de modelos locales
- **Alternativa**: Los modelos `deepseek-coder*` ahora se enrutan a DeepSeek API
- **Recomendaci√≥n**: Usar `qwen2.5:7b-instruct` para tareas locales o DeepSeek API para c√≥digo

## Modelos DeepSeek API

### DeepSeek Chat
- **Contexto m√°ximo**: 128,000 tokens
- **Generaci√≥n m√°xima**: 4,000 tokens
- **Caracter√≠sticas**: Modelo general de chat

### DeepSeek Reasoner
- **Contexto m√°ximo**: 128,000 tokens
- **Generaci√≥n m√°xima**: 16,000 tokens
- **Caracter√≠sticas**: Modelo de razonamiento mejorado

## Modelos con Visi√≥n

### üñºÔ∏è Visi√≥n Unificada con Gemini
**Todos los modelos** (DeepSeek y Ollama) ahora usan **Gemini 2.5 Flash** para an√°lisis de im√°genes:
- **Procesamiento universal**: Cualquier modelo que pase por el proxy tiene visi√≥n habilitada
- **An√°lisis de im√°genes**: Procesado por Gemini (hasta 10MB por imagen)
- **Cach√© contextual**: Hash SHA-256 para evitar llamadas repetidas
- **Prompt adaptativo**: Se ajusta al contexto de la pregunta del usuario

### üîÑ Enrutamiento Inteligente
El proxy detecta autom√°ticamente el destino basado en el modelo solicitado:

```typescript
// Ejemplo de enrutamiento:
"vision-dsk-chat" ‚Üí DeepSeek API (con visi√≥n Gemini)
"qwen2.5:7b-instruct" ‚Üí Ollama local (con visi√≥n Gemini)
"deepseek-coder" ‚Üí DeepSeek API (con visi√≥n Gemini) - Ahora enruta a DeepSeek
```

### üìä Modelos Disponibles en el Proxy
El proxy expone **8 modelos** con visi√≥n:

| Tipo | Modelos Proxy | Modelo Destino | Contexto | Output | Visi√≥n |
|------|---------------|----------------|----------|--------|--------|
| **DeepSeek Chat** | `vision-dsk-chat`, `deepseek-vision-chat` | `deepseek-chat` | 128K | 8K | ‚úÖ |
| **DeepSeek Reasoner** | `vision-dsk-reasoner`, `deepseek-vision-reasoner` | `deepseek-reasoner` | 128K | 64K | ‚úÖ |
| **Qwen2.5** | `qwen2.5-instruct`, `qwen2.5-7b-instruct`, `qwen2.5`, `qwen2.5:7b-instruct` | `qwen2.5:7b-instruct` | 131K | 8K | ‚úÖ |
| **DeepSeek Coder** | `deepseek-coder-instruct`, `deepseek-coder-6.7b-instruct`, `deepseek-coder`, `deepseek-coder:6.7b-instruct-q8_0` | `deepseek-chat` (API) | 128K | 8K | ‚úÖ |

## Configuraci√≥n Recomendada

### Para Qwen2.5:
```json
{
  "context": 131072,
  "output": 8192
}
```

### Para DeepSeek Coder (ahora via API):
```json
{
  "context": 128000,
  "output": 8000
}
```

### Para DeepSeek API:
```json
{
  "context": 128000,
  "output": 4000  // 16000 para Reasoner
}
```

## Notas Importantes

1. **Los l√≠mites de Ollama** son los m√°ximos te√≥ricos seg√∫n documentaci√≥n oficial
2. **En la pr√°ctica**, el rendimiento puede variar seg√∫n:
   - Hardware disponible
   - Quantizaci√≥n del modelo (Q8_0, Q4_K_M, etc.)
   - Configuraci√≥n de memoria
3. **Para uso √≥ptimo**:
   - Qwen2.5: Ideal para tareas de texto largo y multiling√ºes (local)
   - DeepSeek Coder: Ahora usa DeepSeek API para programaci√≥n y c√≥digo
   - DeepSeek API: Mejor para tareas generales y de programaci√≥n con visi√≥n

## Verificaci√≥n

Para verificar los modelos instalados en Ollama:
```bash
ollama list
ollama show qwen2.5:7b-instruct --modelfile | head -10
```

**Nota**: `deepseek-coder:6.7b-instruct-q8_0` ya no es necesario para el proxy, pero puede mantenerse instalado para uso directo con Ollama.