# L√≠mites de Contexto de Modelos

## Modelos Ollama Locales

### Qwen2.5:7B-Instruct
- **Contexto m√°ximo**: 131,072 tokens (128K)
- **Generaci√≥n m√°xima**: 8,192 tokens
- **Fuente**: [Documentaci√≥n oficial Qwen2.5](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
- **Caracter√≠sticas**: 
  - Soporta hasta 128K tokens con YaRN
  - Mejorado para seguir instrucciones
  - Excelente en generaci√≥n de texto largo
  - Soporte multiling√ºe (29+ idiomas)

### DeepSeek-Coder:6.7B-Instruct-Q8_0
- **Contexto m√°ximo**: 16,384 tokens (16K)
- **Generaci√≥n m√°xima**: 4,096 tokens (estimado)
- **Fuente**: [Config.json del modelo](https://huggingface.co/deepseek-ai/DeepSeek-Coder-6.7B-Instruct/raw/main/config.json)
- **Caracter√≠sticas**:
  - Especializado en c√≥digo
  - Entrenado con 87% c√≥digo, 13% lenguaje natural
  - Ventana de 16K para completado de c√≥digo a nivel de proyecto
  - Tarea de rellenar espacios en blanco

## Modelos DeepSeek API

### DeepSeek Chat
- **Contexto m√°ximo**: 128,000 tokens
- **Generaci√≥n m√°xima**: 4,000 tokens
- **Caracter√≠sticas**: Modelo general de chat

### DeepSeek Reasoner
- **Contexto m√°ximo**: 128,000 tokens
- **Generaci√≥n m√°xima**: 16,000 tokens
- **Caracter√≠sticas**: Modelo de razonamiento mejorado

## Modelos con Visi√≥n

### üñºÔ∏è Visi√≥n Unificada con Gemini
**Todos los modelos** (DeepSeek y Ollama) ahora usan **Gemini 2.5 Flash** para an√°lisis de im√°genes:
- **Procesamiento universal**: Cualquier modelo que pase por el proxy tiene visi√≥n habilitada
- **An√°lisis de im√°genes**: Procesado por Gemini (hasta 10MB por imagen)
- **Cach√© contextual**: Hash SHA-256 para evitar llamadas repetidas
- **Prompt adaptativo**: Se ajusta al contexto de la pregunta del usuario

### üîÑ Enrutamiento Inteligente
El proxy detecta autom√°ticamente el destino basado en el modelo solicitado:

```typescript
// Ejemplo de enrutamiento:
"vision-dsk-chat" ‚Üí DeepSeek API (con visi√≥n Gemini)
"qwen2.5-instruct" ‚Üí Ollama local (con visi√≥n Gemini)
"deepseek-coder" ‚Üí Ollama local (con visi√≥n Gemini)
```

### üìä Modelos Disponibles en el Proxy
El proxy expone **10 modelos** con visi√≥n:

| Tipo | Modelos Proxy | Modelo Destino | Contexto | Output | Visi√≥n |
|------|---------------|----------------|----------|--------|--------|
| **DeepSeek Chat** | `vision-dsk-chat`, `deepseek-vision-chat` | `deepseek-chat` | 128K | 8K | ‚úÖ |
| **DeepSeek Reasoner** | `vision-dsk-reasoner`, `deepseek-vision-reasoner` | `deepseek-reasoner` | 128K | 64K | ‚úÖ |
| **Qwen2.5** | `qwen2.5-instruct`, `qwen2.5-7b-instruct`, `qwen2.5` | `qwen2.5:7b-instruct` | 131K | 8K | ‚úÖ |
| **DeepSeek Coder** | `deepseek-coder-instruct`, `deepseek-coder-6.7b-instruct`, `deepseek-coder` | `deepseek-coder:6.7b-instruct-q8_0` | 16K | 4K | ‚úÖ |

## Configuraci√≥n Recomendada

### Para Qwen2.5:
```json
{
  "context": 131072,
  "output": 8192
}
```

### Para DeepSeek Coder:
```json
{
  "context": 16384,
  "output": 4096
}
```

### Para DeepSeek API:
```json
{
  "context": 128000,
  "output": 4000  // 16000 para Reasoner
}
```

## Notas Importantes

1. **Los l√≠mites de Ollama** son los m√°ximos te√≥ricos seg√∫n documentaci√≥n oficial
2. **En la pr√°ctica**, el rendimiento puede variar seg√∫n:
   - Hardware disponible
   - Quantizaci√≥n del modelo (Q8_0, Q4_K_M, etc.)
   - Configuraci√≥n de memoria
3. **Para uso √≥ptimo**:
   - Qwen2.5: Ideal para tareas de texto largo y multiling√ºes
   - DeepSeek Coder: Especializado en programaci√≥n y c√≥digo
   - DeepSeek API: Mejor para tareas generales con visi√≥n

## Verificaci√≥n

Para verificar los modelos instalados en Ollama:
```bash
ollama list
ollama show qwen2.5:7b-instruct --modelfile | head -10
ollama show deepseek-coder:6.7b-instruct-q8_0 --modelfile | head -10
```