# L√≠mites de Contexto de Modelos

## Modelos OpenCode (OpenAI API)

### DeepSeek Chat

- **Contexto m√°ximo**: 128,000 tokens
- **Generaci√≥n m√°xima**: 8,000 tokens
- **Caracter√≠sticas**: Modelo general de chat

### DeepSeek Reasoner

- **Contexto m√°ximo**: 128,000 tokens
- **Generaci√≥n m√°xima**: 64,000 tokens
- **Caracter√≠sticas**: Modelo de razonamiento mejorado

## Modelos con Visi√≥n (OpenCode)

### üñºÔ∏è Modelos Multimodales con Gemini 2.5 Flash Lite

Todos los modelos ahora usan **Gemini 2.5 Flash Lite** para an√°lisis multimodal avanzado:

- **Procesamiento Universal**: Cualquier modelo que pase por el proxy tiene multimodalidad habilitada.
- **An√°lisis de Im√°genes**: OCR superior y descripci√≥n visual.
- **An√°lisis de Audio/Video**: Transcripci√≥n y descripci√≥n contextual (MP3/MP4 validados).
- **Soporte de PDFs**: Sistema h√≠brido. Gemini soporta PDFs nativamente para an√°lisis de tablas/gr√°ficos complejos. El proxy a√±ade procesamiento local para archivos < 1MB por velocidad y costo.
- **Cach√© Contextual**: Hash SHA-256(content + pregunta) para evitar llamadas Gemini repetidas.
- **L√≠mite por archivo**: **50MB** (Con validaci√≥n HEAD previa para evitar descargas innecesarias).

### üîÑ Enrutamiento Inteligente

El proxy detecta autom√°ticamente el destino basado en el modelo solicitado:

```typescript
"deepseek-multimodal-chat"     ‚Üí DeepSeek Chat (v3.2) + Gemini Percepci√≥n
"deepseek-multimodal-reasoner" ‚Üí DeepSeek Reasoner (r1) + Gemini Percepci√≥n
```

### üìä Modelos Disponibles en el Proxy

| Modelo Proxy                   | Modelo Destino      | Contexto (Input) | Salida (Output) | Modalidades                       |
| :----------------------------- | :------------------ | :--------------- | :-------------- | :-------------------------------- |
| `deepseek-multimodal-chat`     | `deepseek-chat`     | 100K             | 8K              | ‚úÖ Text, Image, Audio, Video, PDF |
| `deepseek-multimodal-reasoner` | `deepseek-reasoner` | 100K             | 64K             | ‚úÖ Text, Image, Audio, Video, PDF |

## Modelos Claude Code (Anthropic)

Los clientes Anthropic usan `/v1/messages` y estos alias:

| Modelo Claude | Modelo Interno             | Routing Estrat√©gico |
| :------------ | :------------------------- | :-- |
| `haiku`       | `gemini-direct`            | **Bypass total**: Todo va a Gemini, sin DeepSeek |
| `sonnet`      | `deepseek-multimodal-chat` | **Inteligente**: Texto ‚Üí DeepSeek, Multimodal ‚Üí Gemini ‚Üí DeepSeek |
| `opus`        | `deepseek-multimodal-reasoner` | **Inteligente**: Texto ‚Üí DeepSeek, Multimodal ‚Üí Gemini ‚Üí DeepSeek |

### **Routing Inteligente por Modelo**

- **Haiku**: Estrategia `gemini-direct` para m√°xima velocidad y econom√≠a
- **Sonnet/Opus**: Estrategia `deepseek-routing` con an√°lisis de contenido:
  - **Texto/c√≥digo**: DeepSeek directo
  - **Im√°genes/audio/video**: Gemini ‚Üí DeepSeek
  - **PDFs**: Procesamiento local o Gemini ‚Üí DeepSeek

### Configuraci√≥n de L√≠mites (v√≠a .env)

Los l√≠mites son personalizables para adaptarse a las cuotas de tu API de DeepSeek:

- **Chat**: 100,000 contextual / 8,000 generaci√≥n.
- **Reasoner**: 100,000 contextual / 64,000 generaci√≥n.

```json
{
  "context": 100000,
  "output": 8000,
  "cost": {
    "input": 0.27,
    "output": 1.1
  }
}
```

### Para DeepSeek Reasoner:

```json
{
  "context": 100000,
  "output": 64000,
  "cost": {
    "input": 0.55,
    "output": 2.19
  }
}
```
