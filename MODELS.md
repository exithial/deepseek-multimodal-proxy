# L√≠mites de Contexto de Modelos

## Modelos DeepSeek API

### DeepSeek Chat

- **Contexto m√°ximo**: 128,000 tokens
- **Generaci√≥n m√°xima**: 4,000 tokens
- **Caracter√≠sticas**: Modelo general de chat

### DeepSeek Reasoner

- **Contexto m√°ximo**: 128,000 tokens
- **Generaci√≥n m√°xima**: 16,000 tokens
- **Caracter√≠sticas**: Modelo de razonamiento mejorado

## Modelos con Visi√≥n

### üñºÔ∏è Visi√≥n Unificada con Gemini

**Todos los modelos** (DeepSeek y Ollama) ahora usan **Gemini 2.5 Flash** para an√°lisis de im√°genes:

- **Procesamiento universal**: Cualquier modelo que pase por el proxy tiene visi√≥n habilitada
- **An√°lisis de im√°genes**: Procesado por Gemini (hasta 10MB por imagen)
- **Cach√© contextual**: Hash SHA-256 para evitar llamadas repetidas
- **Prompt adaptativo**: Se ajusta al contexto de la pregunta del usuario

### üîÑ Enrutamiento Inteligente

El proxy detecta autom√°ticamente el destino basado en el modelo solicitado:

```typescript
// Ejemplo de enrutamiento:
"vision-dsk-chat" ‚Üí DeepSeek API (con visi√≥n Gemini)
"qwen2.5:7b-instruct" ‚Üí Ollama local (con visi√≥n Gemini)
"deepseek-coder" ‚Üí DeepSeek API (con visi√≥n Gemini) - Ahora enruta a DeepSeek
```

### üìä Modelos Disponibles en el Proxy

El proxy expone **8 modelos** con visi√≥n:

| Tipo                  | Modelos Proxy                                     | Modelo Destino      | Contexto | Output | Visi√≥n |
| --------------------- | ------------------------------------------------- | ------------------- | -------- | ------ | ------ |
| **DeepSeek Chat**     | `vision-dsk-chat`, `deepseek-vision-chat`         | `deepseek-chat`     | 128K     | 8K     | ‚úÖ     |
| **DeepSeek Reasoner** | `vision-dsk-reasoner`, `deepseek-vision-reasoner` | `deepseek-reasoner` | 128K     | 64K    | ‚úÖ     |

## Configuraci√≥n Recomendada

### Para DeepSeek Coder (ahora via API):

```json
{
  "context": 128000,
  "output": 8000
}
```

### Para DeepSeek API:

```json
{
  "context": 128000,
  "output": 4000 // 16000 para Reasoner
}
```
